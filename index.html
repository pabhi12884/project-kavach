<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project Kavach</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
  <header>
    <img src="logo.png" alt="Project Kavach Logo" class="logo"/>
    <h1>Project Kavach — The AI Safety Shield</h1>
  </header>

  <main>
    <section>
      <h2>What Is This Project Kavach</h2>
      <p> Project Kavacha is a fully automated system designed to test and verify whether AI models like ChatGPT, Claude, or LLaMA are following safety rules, ethics, and legal guidelines. It simulates risky user inputs, evaluates AI outputs, and flags dangerous or unethical behavior.</p>
    </section>

    <section>
      <h2>Our Goals</h2>
      <ul>
        <li>Test AI for bias, safety, and ethics</li>
        <li>Enable trust in AI</li>
        <li>Make AI Reponsible, Ethical, Safe</li>
        <li>Create open-source tool to verify AI guardrails</li>
      </ul>
    </section>

    <section>
        <h2>Our Outcome</h2>
        <p> We aim to build an open, scalable, and security-first toolset designed to evaluate and audit AI models. Our focus is on verifying whether AI systems follow essential safety, ethical, and privacy standards — before deployment or release. This ensures that responsible AI development is measurable and enforceable, not left to assumption or chance.</p>
      </section>

    <section>
      <h2>Why Project Kavach?</h2>
      <p>‘Kavach’ means ‘shield’ in Sanskrit — we aim to shield users and systems from AI misuse and mistakes.</p>
    </section>
  </main>

  <footer>
    <p>© 2025 Project Kavach. All rights reserved.</p>
  </footer>
</body>
</html>
